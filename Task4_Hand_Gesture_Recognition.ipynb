{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \u270b Hand Gesture Recognition Using CNN\n", "Prodigy Infotech \u2013 Task 04\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\n\ndata = []\nlabels = []\n\ndataset_path = 'leapGestRecog'\n\nfor folder in os.listdir(dataset_path):\n    folder_path = os.path.join(dataset_path, folder)\n    if not os.path.isdir(folder_path):\n        continue\n    label = int(folder)\n    for img in os.listdir(folder_path):\n        img_path = os.path.join(folder_path, img)\n        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        image = cv2.resize(image, (64,64))\n        data.append(image)\n        labels.append(label)\n\ndata = np.array(data).reshape(-1,64,64,1) / 255.0\nlabels = to_categorical(np.array(labels))\n\nX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n\nmodel = Sequential([\n    Conv2D(32,(3,3),activation='relu',input_shape=(64,64,1)),\n    MaxPooling2D(2,2),\n    Conv2D(64,(3,3),activation='relu'),\n    MaxPooling2D(2,2),\n    Flatten(),\n    Dense(128,activation='relu'),\n    Dropout(0.3),\n    Dense(labels.shape[1],activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=5, validation_split=0.2)\n\nloss, acc = model.evaluate(X_test, y_test)\nprint(\"Accuracy =\", acc)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}